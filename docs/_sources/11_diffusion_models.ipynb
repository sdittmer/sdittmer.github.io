{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e892593-356b-41f0-bbf0-b0962d5376cb",
   "metadata": {},
   "source": [
    "# Score-based models\n",
    "\n",
    "* Question: Can we create samples that are as nice as GAN samples without unstable adversarial training?\n",
    "\n",
    "Like in a GAN, the basic idea is to create samples from some distribution $p_X$ from samples $p_Z=\\mathcal{N}(0,\\mathbb{1})$. Unlike GANs the idea to let the image slowly emerge from the noise.\n",
    "\n",
    "\n",
    "```{figure} images/duoduo.jpg\n",
    "---\n",
    "height: 100px\n",
    "---\n",
    "We want a model that can go iteratively from right to left. [Source.](https://yang-song.net/blog/2021/score/)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44bb80-6ca3-4c6c-baff-ac57b4baccf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Score-based models ([source](https://yang-song.net/blog/2021/score/), [source](https://arxiv.org/abs/2011.13456))\n",
    "\n",
    "For Score-based models I highly recommend [this blog post](https://yang-song.net/blog/2021/score/) by Yang Song. This section is heavily based on it.\n",
    "\n",
    "The basic idea is to step by step reverse the process of adding more and more Gaussian noise to a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322eebb-9f13-4b71-bf46-17063e1b8edd",
   "metadata": {},
   "source": [
    "### First of two ingredients: Langevin dynamics\n",
    "\n",
    "The **Langevin dynamic** is given by the iteration\n",
    "\n",
    "$$x_{i+1} = x_i + \\epsilon \\nabla \\log p(x) + \\sqrt{2\\epsilon} z_i, \\ i=0\\cdots K$$\n",
    "\n",
    "where $p$ some \"sufficiently nice\" probabiltiy density $z_i\\sim\\mathcal{N}(0,\\mathbb{1})$ and some arbitrary $x_0$. For $\\epsilon\\to0$ and $K\\to\\infty$ this iteration produces a sample from $p(x)$.\n",
    "\n",
    "* In practice we can choose $\\epsilon$ small and $K$ large.\n",
    "\n",
    "```{figure} images/langevin.gif\n",
    "---\n",
    "height: 200px\n",
    "---\n",
    "Using Langevin dynamics to sample from a mixture of two Gaussians. [Source.](https://yang-song.net/blog/2021/score/)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102af623-6ff1-451d-8d3c-272ad47d33e5",
   "metadata": {},
   "source": [
    "### Second of two ingredients: Tweedie's formula\n",
    "\n",
    "**Tweedie's formula** tells us that\n",
    "\n",
    "$$\\sigma^2 \\nabla \\log p_{\\sigma}(x^\\delta) = \\mathbb{E}(x|x^\\delta, \\sigma) - x^\\delta,$$\n",
    "\n",
    "where $\\eta\\sim p_\\eta = \\mathcal{N}(0,\\mathbb{1})$ and the distribution $p_{\\sigma}(x^\\delta)$ is given by the samples $x^\\delta = x+\\sigma\\eta$, for some given $\\sigma>0$.\n",
    "\n",
    "We can interpret $\\mathbb{E}(x|x+\\sigma\\eta, \\sigma)$ as the \"perfect denoiser\" for the noise level $\\sigma$.\n",
    "\n",
    "Using $\\mathbb{E}(\\eta|x^\\delta, \\sigma) = x^\\delta - \\mathbb{E}(x|x^\\delta, \\sigma)$ Tweedie's formula gives us\n",
    "\n",
    "$$-\\sigma^2 \\nabla \\log p_{\\sigma}(x^\\delta) = \\mathbb{E}(\\eta|x^\\delta, \\sigma).$$\n",
    "\n",
    "**Here is what that means in practice.** Given a dataset $\\mathcal{D}=\\{x_i\\}_i$, we can training a network\n",
    "\n",
    "$$s_\\theta:\\mathbb{R}^n\\times\\mathbb{R}_{\\ge0}\\to\\mathbb{R}^n$$\n",
    "\n",
    "via the loss\n",
    "\n",
    "$$L_\\sigma(\\theta) = \\frac{1}{|\\mathcal{D}|}\\sum_{x\\sim \\mathbb{D}} \\mathbb{E}_{\\eta\\sim \\mathcal{N}(0,\\mathbb{1})} \\|s(x + \\sigma \\eta, \\sigma) - \\eta\\|^2$$\n",
    "\n",
    "to ideally give us\n",
    "\n",
    "$$s(x+\\sigma\\eta,\\sigma) = \\mathbb{E}\\left[\\eta|x+\\sigma\\eta,\\sigma\\right] = -\\sigma^2 \\nabla \\log p_{\\sigma}(x^\\delta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84148b4c-fb8d-4900-80ce-670e0d52fb9c",
   "metadata": {},
   "source": [
    "### Putting the ingredients together\n",
    "\n",
    "We will train model $s$ based on the \"Tweedie\" loss from to work for different noise levels, e.g., via the loss\n",
    "\n",
    "$$L(\\theta) = \\sum_{i=1}^N \\sigma_i^2 L_{\\sigma_i}(\\theta)$$\n",
    "\n",
    "where $\\sigma_i < \\sigma_{i+1}$ and the $\\sigma_i$ usually follow some geometric progression.\n",
    "\n",
    "With that model we can now run a slightly modified Lagavin dynamic\n",
    "\n",
    "$$x_{i+1} = x_i + \\tilde \\epsilon_i \\nabla\\log p_{\\sigma_{N-i}}(x_i) + \\sqrt{2\\epsilon_i} z_i$$\n",
    "\n",
    "$$= x_i - \\epsilon_i \\ s(x_i, \\sigma_{L-i}) + \\sqrt{2\\epsilon_i} z_i$$\n",
    "\n",
    "for $N$ iterations (usually on the order of $1000$). Here on picks $\\epsilon_i = \\epsilon \\sigma_i^2 / \\sigma_L^2$ for some $\\epsilon>0$. One often also holds the $\\sigma_i$ and $\\epsilon_i$ constant over a number of iterations before deceasing its index, i.e., for some $k$ one uses $\\sigma_j$, with $j  = \\lceil \\frac{L-i}{k} \\rceil k$ and use $Nk$ iterations.\n",
    "\n",
    "We can use this iteration to approximate samples from $p_X$.\n",
    "\n",
    "```{note}\n",
    "Using Langevin dynamic with this decaying noise level is called **annealed Langevin dynamics**.\n",
    "```\n",
    "\n",
    "\n",
    "```{figure} images/ald.gif\n",
    "---\n",
    "height: 200px\n",
    "---\n",
    "Langevin dynamics if we would run it for different $\\sigma_i$. [Source](https://yang-song.net/blog/2021/score/)\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "Question: Why do we use annealed Langevin dynamics? Shouldn't a single small $\\sigma_i$ be enough? In theory, yes. In practice the network $s$ only works for samples similar to the ones it has seen during training. If we therefore would only use a single small $\\sigma_i$ the network would not know how to deal with extremly noisy samples and therefore would not work for the earily $x_i$.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
