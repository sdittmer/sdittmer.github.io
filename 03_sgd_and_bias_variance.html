
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>SGD guarantees &amp; the bias–variance trade-off &#8212; A Primer on the Mathematical Foundations of Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '03_sgd_and_bias_variance';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Invariance &amp; robustness" href="04_invariance_robustness.html" />
    <link rel="prev" title="Deep vs shallow" href="02_deep_vs_shallow.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="None">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="intro.html">

  
  
  
  
  
  
  

  
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="00_the_mlp.html">
                        The multilayer perceptron
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="01_mlp_universal_approx.html">
                        The multilayer perceptron, a universal approximator
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="02_deep_vs_shallow.html">
                        Deep vs shallow
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        SGD guarantees & the bias–variance trade-off
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="04_invariance_robustness.html">
                        Invariance & robustness
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="05_equivariance_and_cnns.html">
                        Equivariance & convolutional neural networks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="06_loss_landscape_normalization_resnets.html">
                        The loss landscape: Normalization & Residual Networks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="07_input_convex_neural_nets.html">
                        Input-convex neural networks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="08_autoencoder.html">
                        Autoencoders
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="09_variational_autoencoder.html">
                        Variational autoencoders
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="10_GANs.html">
                        Generative adversarial networks (GANs)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="11_diffusion_models.html">
                        Score-based models
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="00_the_mlp.html">
                        The multilayer perceptron
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="01_mlp_universal_approx.html">
                        The multilayer perceptron, a universal approximator
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="02_deep_vs_shallow.html">
                        Deep vs shallow
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        SGD guarantees & the bias–variance trade-off
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="04_invariance_robustness.html">
                        Invariance & robustness
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="05_equivariance_and_cnns.html">
                        Equivariance & convolutional neural networks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="06_loss_landscape_normalization_resnets.html">
                        The loss landscape: Normalization & Residual Networks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="07_input_convex_neural_nets.html">
                        Input-convex neural networks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="08_autoencoder.html">
                        Autoencoders
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="09_variational_autoencoder.html">
                        Variational autoencoders
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="10_GANs.html">
                        Generative adversarial networks (GANs)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="11_diffusion_models.html">
                        Score-based models
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="intro.html">

  
  
  
  
  
  
  

  
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    A Primer on the Mathematical Foundations of Deep Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_the_mlp.html">The multilayer perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_mlp_universal_approx.html">The multilayer perceptron, a universal approximator</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_deep_vs_shallow.html">Deep vs shallow</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">SGD guarantees &amp; the bias–variance trade-off</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_invariance_robustness.html">Invariance &amp; robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_equivariance_and_cnns.html">Equivariance &amp; convolutional neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_loss_landscape_normalization_resnets.html">The loss landscape: Normalization &amp; Residual Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_input_convex_neural_nets.html">Input-convex neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_autoencoder.html">Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_variational_autoencoder.html">Variational autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_GANs.html">Generative adversarial networks (GANs)</a></li>

<li class="toctree-l1"><a class="reference internal" href="11_diffusion_models.html">Score-based models</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">


<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://colab.research.google.com/v2/gh/sdittmer/mathematical_foundations_of_deep_learning/master?urlpath=tree/docs/03_sgd_and_bias_variance.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</a>
      
  </ul>
</div>

<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="_sources/03_sgd_and_bias_variance.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>SGD guarantees & the bias–variance trade-off</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sgd-convergence-guarantees">
   SGD convergence guarantees
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-biasvariance-trade-off">
   The bias–variance trade-off
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sgd">
     SGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias-variance">
     Bias-variance
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="sgd-guarantees-the-biasvariance-trade-off">
<h1>SGD guarantees &amp; the bias–variance trade-off<a class="headerlink" href="#sgd-guarantees-the-biasvariance-trade-off" title="Permalink to this headline">#</a></h1>
<p>We will try to answer two:</p>
<ol class="arabic simple">
<li><p>Can we understand what the training process (SGD) does in a simplified setting?</p></li>
<li><p>How to pick a model considering its expected error?</p></li>
</ol>
<section id="sgd-convergence-guarantees">
<h2>SGD convergence guarantees<a class="headerlink" href="#sgd-convergence-guarantees" title="Permalink to this headline">#</a></h2>
<div class="proof definition admonition" id="convexity_def">
<p class="admonition-title"><span class="caption-number">Definition 2 </span></p>
<section class="definition-content" id="proof-content">
<p>We call a function <span class="math notranslate nohighlight">\(f:\mathbb{R}^n \to \mathbb{R}\)</span> convex if for all <span class="math notranslate nohighlight">\(t\in[0,1]\)</span></p>
<div class="math notranslate nohighlight">
\[f(tx + (1-t)y) \le t f(x) + (1-t)f(y) \ \forall x, y.\]</div>
</section>
</div><div class="proof lemma admonition" id="convex_lemma">
<p class="admonition-title"><span class="caption-number">Lemma 4 </span></p>
<section class="lemma-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f:\mathbb{R}^n\to\mathbb{R}\)</span> is differnetiable, it is convex iff</p>
<div class="math notranslate nohighlight">
\[f(x) - f(y) \le \langle \nabla f(x), x-y\rangle \forall x, y.\]</div>
</section>
</div><div class="proof lemma admonition" id="blg_lemma">
<p class="admonition-title"><span class="caption-number">Lemma 5 </span> (A bound on functions with Lipschitz gradient)</p>
<section class="lemma-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f:\mathbb{R}^n\to\mathbb{R}\)</span> has an L-Lipschitz gradient, we have</p>
<div class="math notranslate nohighlight">
\[f(y) \le f(x) + \langle \nabla f(x), y-x\rangle + \frac{L}{2}\|y-x\|_2^2 \; \forall x, y.\]</div>
</section>
</div><div class="dropdown admonition">
<p class="admonition-title">Click for proof.</p>
<div class="proof admonition" id="proof">
<p>Proof. Sketch:</p>
<ol class="arabic simple">
<li><p>Write out the first/second order Taylor expansion.</p></li>
<li><p>Use the Taylor inequality.</p></li>
<li><p>Bound the Taylor inequality using <span class="math notranslate nohighlight">\(L\)</span>.</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</div>
<div class="proof theorem admonition" id="sgd_thm">
<p class="admonition-title"><span class="caption-number">Theorem 7 </span> (Convergence of SGD)</p>
<section class="theorem-content" id="proof-content">
<p>We assume that <span class="math notranslate nohighlight">\(f:\mathbb{R}^n\to\mathbb{R}\)</span> has an L-Lipschitz gradient, and is convex. We also assume that we use stochastic gradients <span class="math notranslate nohighlight">\(v_k\)</span> from an unbiased estimator with a bounded variance</p>
<div class="math notranslate nohighlight">
\[\mbox{Var}(v_k) = \mathbb{E}\|v_k\|_2^2 - \|\mathbb{E}v_k\|_2^2 \le \sigma^2.\]</div>
<p>Then, for any SGD iteration <span class="math notranslate nohighlight">\(k\)</span> with step size <span class="math notranslate nohighlight">\(t_k=t\le \frac{1}{L}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E} f(\overline x_k) \le f(x^*) + \frac{\|x_0-x^*\|_2^2}{2tk} + t\sigma^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline x_k = \frac{1}{k} \sum_{i=1}^{k}x_{i}\)</span> and <span class="math notranslate nohighlight">\(x^* = \arg\min_x f(x)\)</span>.</p>
<p>Further, we can <em>expect</em> to find an <span class="math notranslate nohighlight">\(\epsilon/2\)</span> optimal value (in terms of its loss) within</p>
<div class="math notranslate nohighlight">
\[k = \frac{(2\sigma^2 + \|x_0-x^*\|_2^2)^2}{\epsilon^2}\]</div>
<p>iterations, by setting <span class="math notranslate nohighlight">\(t = \frac{1}{\sqrt{k}}\)</span>. Here we assume <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{k}} \le \frac{1}{L}\)</span>.</p>
</section>
</div><div class="dropdown admonition">
<p class="admonition-title">Click for proof.</p>
<div class="proof admonition" id="proof">
<p>Proof. The lemma above gives us</p>
<div class="math notranslate nohighlight">
\[f(x_{i+1}) \le f(x_i) + \langle\nabla f(x_i), x_{i+1} - x_i\rangle + \frac{L}{2}\|x_{i+1} - x_i\|_2^2\]</div>
<div class="math notranslate nohighlight">
\[= f(x_i) - t\langle\nabla f(x_i),v_i\rangle + \frac{Lt^2}{2}\|v_i\|_2^2.\]</div>
<p>Taking the expectation and using the bound on <span class="math notranslate nohighlight">\(t\)</span> gives us</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}f(x_{i+1}) \le f(x_i) - t \|\nabla f(x_i)\|_2^2 + \frac{Lt^2}{2} [ \|\nabla f(x_i)\|_2^2 + \mbox{Var}(v_i)]\]</div>
<div class="math notranslate nohighlight">
\[\le f(x_i) - t (1-\frac{Lt}{2})\|\nabla f(x_i)\|_2^2 + \frac{Lt^2}{2}\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[\le f(x_i) - \frac{t}{2} \|\nabla f(x_i)\|_2^2 + \frac{t}{2}\sigma^2.\]</div>
<p>Via convexity we know</p>
<div class="math notranslate nohighlight">
\[f(x^*) + \langle\nabla f(x_i), x_i - x^*\rangle \ge f(x_i)\]</div>
<p>and therefore we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}f(x_{i+1}) \le f(x^*) + \langle\nabla f(x_i), x_i - x^*\rangle - \frac{t}{2} \|\nabla f(x_i)\|_2^2 + \frac{t}{2}\sigma^2.\]</div>
<p>Since we assume an unbiased estimator this can be written as</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}f(x_{i+1}) \le f(x^*) + \langle\mathbb{E} v_i, x_i - x^*\rangle - \frac{t}{2} \|\mathbb{E} v_i\|_2^2 + \frac{t}{2}\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[\mathbb{E}f(x_{i+1}) \le f(x^*) + \langle\mathbb{E} v_i, x_i - x^*\rangle - \frac{t}{2} (\mathbb{E}\|v_k\|_2^2 - \text{Var}(v_k)) + \frac{t}{2}\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[= f(x^*) + \mathbb{E} \left[\langle v_i, x_i - x^*\rangle - \frac{t}{2} \| v_i\|_2^2\right] + t\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[= f(x^*) + \mathbb{E} \left[\frac{1}{2t}\left(\|x_i-x^*\|_2^2 - \|x_i-x^*-tv_i\|_2^2\right)\right] + t\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[= f(x^*) + \mathbb{E} \left[\frac{1}{2t}\left(\|x_i-x^*\|_2^2 - \|x_{i+1}-x^*\|_2^2\right)\right] + t\sigma^2.\]</div>
<p>By taking a sum and noticing the canceling terms we have</p>
<div class="math notranslate nohighlight">
\[\sum_{i=0}^{k-1} \mathbb{E} f(x_{i+1}) - f(x^*) \le \frac{1}{2t} \left(\|x_0 - x^*\|_2^2 - \mathbb{E}\|x_k-x^*\|_2^2\right) + kt\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[\le \frac{\|x_0-x^*\|_2^2}{2t} + kt\sigma^2\]</div>
<p>and thereby</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\frac{1}{k}\sum_{i=0}^{k-1} f(x_{i+1}) \le f(x^*) + \frac{\|x_0-x^*\|_2^2}{2tk} + t\sigma^2.\]</div>
<p>Jensen’s inequality, i.e., convexity, now gives us</p>
<div class="math notranslate nohighlight">
\[\mathbb{E} f(\overline x_k) = \mathbb{E} f(\frac{1}{k}\sum_{i=0}^{k-1}x_{i+1}) \le f(x^*) + \frac{\|x_0-x^*\|_2^2}{2tk} + t\sigma^2.\]</div>
<p>We will leave the “number of iterations” part of the result as an exercise.</p>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“Hard” convergence results of SGD usually require a non-constant step size, e.g., <span class="math notranslate nohighlight">\(\mbox{step size} = \frac{1}{\mbox{current iteration}}\)</span>.</p>
</div>
</section>
<section id="the-biasvariance-trade-off">
<h2>The bias–variance trade-off<a class="headerlink" href="#the-biasvariance-trade-off" title="Permalink to this headline">#</a></h2>
<p>We assume a regression dataset</p>
<div class="math notranslate nohighlight">
\[\mathscr{D} =\{(x_i,y_i)\}_{i=1}^N \subset \mathbb{R}^n \times \mathbb{R}\]</div>
<p>of realizations <span class="math notranslate nohighlight">\((x_i, y_i)\sim p_{X,Y}\)</span>.</p>
<p>And we assume that the labels, <span class="math notranslate nohighlight">\(y\)</span>, are not deterministicly determined by the features <span class="math notranslate nohighlight">\(x\)</span>.
As this means that we can have mutliple labels, it makes sense to think about the expected label for a given feature vector <span class="math notranslate nohighlight">\(x\)</span>.</p>
<ul class="simple">
<li><p>The <strong>expected label</strong> is defined as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\overline y(x) = \mathbb{E}_{y|x} Y = \int_\mathbb{R} y\ d\mathbb{P}(y|x) = \int_\mathbb{R} y\ p(y|x) dy.\]</div>
<ul class="simple">
<li><p>We will now also define the trained model based on some deterministic (in practice you could use a fixed seed) approach <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> and the dataset as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[f_\mathcal{D} = \mathcal{A}(\mathcal{D}).\]</div>
<ul class="simple">
<li><p>As <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is randomly drawen from <span class="math notranslate nohighlight">\(p_{X,Y}\)</span> we can also think about the <strong>expected model</strong> we get from an approach <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>. The approach describes the whole process of producing the model. This includes the network architecture, initialization, optimizer, learning rate, … We define the <strong>expected model</strong> as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\overline f = \mathbb{E}_\mathcal{D} f_\mathcal{D} = \int_{\mathbb{R}^{(n+1) \times N}} f_\mathcal{D} \ d\mathbb{P}(\mathcal{D}).\]</div>
<ul class="simple">
<li><p>Having these concepts in place allows us to think about the <strong>expected test error</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{(x,y)\sim p_{x,y}, \mathcal{D}\sim p_{X,Y}^N} \left(f_\mathcal{D}(x) - y\right)^2.\]</div>
<div class="proof theorem admonition" id="bv_thm">
<p class="admonition-title"><span class="caption-number">Theorem 8 </span> (Bias-variance decomposition)</p>
<section class="theorem-content" id="proof-content">
<p>We can decompose the expected test error in the following way:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}_{\substack{(x,y)\sim p_{x,y}\\ \mathcal{D}\sim p_{X,Y}^N}} \left(f_\mathcal{D}(x) - y\right)^2 = \mathbb{E}_{x, \mathcal{D}} (f_\mathcal{D}(x) - \overline f(x))^2 + \mathbb{E}_x (\overline f(x) - \overline y(x))^2 + \mathbb{E}_{x,y} (\overline y(x) - y)^2.\end{split}\]</div>
<p>We can interpret these terms of the sum as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{x, D} (f_\mathcal{D}(x) - \overline f(x))^2\)</span></p>
<ul>
<li><p>The <strong>variance</strong> our approach has for different datasets of size <span class="math notranslate nohighlight">\(N\)</span> from <span class="math notranslate nohighlight">\(p_{X,Y}\)</span>.</p></li>
<li><p>I.e., if we would have had a different dataset, how different would we expect our model to be.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_x (\overline f(x) - \overline y(x))^2\)</span></p>
<ul>
<li><p>The <strong>bias</strong> our approach has for different datasets of size <span class="math notranslate nohighlight">\(N\)</span> from <span class="math notranslate nohighlight">\(p_{X,Y}\)</span>.</p></li>
<li><p>I.e., does our approach produce models with a systematic error, e.g., values that are always too large.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{x,y} (\overline y(x) - y)^2\)</span></p>
<ul>
<li><p>This is the <strong>noise</strong> inherent to the label due to its non-deterministic nature w.r.t. the features.</p></li>
<li><p>I.e., the stuff that is impossible to know.</p></li>
</ul>
</li>
</ul>
</section>
</div><div class="dropdown admonition">
<p class="admonition-title">Click for proof.</p>
<div class="proof admonition" id="proof">
<p>Proof. We have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{x,y,D}\left[f_{D}(x) - y\right]^{2}
= \mathbb{E}_{x,y,D}\left[\left(f_{D}(x) - \overline f(x)\right) + \left(\overline f(x) - y\right)\right]^{2}\nonumber\]</div>
<div class="math notranslate nohighlight">
\[
= \mathbb{E}_{x, D}(\overline f_{D}(x) - \overline f(x))^{2} + \mathbb{E}_{x, y} \left(\overline f(x) - y\right)^{2} + 2 \mathbb{E}_{x, y, D} \left(f_{D}(x) - \overline f(x)\right)\left(\overline f(x) - y\right)
\]</div>
<p>By splitting up <span class="math notranslate nohighlight">\(\mathbb{E}_{x, y, D} = \mathbb{E}_{x, y}\mathbb{E}_{D}\)</span> we see that the last term vanishes.</p>
<p>Now we only need to show that the second term decomposes into the desired outcome.
By expanding we find</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{x,y} (\overline f(x) - y)^2 = \mathbb{E}_{x,y} \left[(\overline f(x) - \overline y(x)) + (\overline y(x) - y)\right]^2\]</div>
<div class="math notranslate nohighlight">
\[= \mathbb{E}_{x,y} (\overline f(x) - \overline y(x))^2 + \mathbb{E}_{x,y} (\overline y(x) - y)^2 + 2\mathbb{E}_{x,y} [\overline f(x) -\overline y(x)][\overline y(x) - y].\]</div>
<p>Again, by decomposing an expectation, this time <span class="math notranslate nohighlight">\(\mathbb{E}_{x,y} = \mathbb{E}_{x}\mathbb{E}_{y|x}\)</span>, we see that the last term vanishes. <span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
</div>
<p>Having a high variance and low bias tends to lead to overfitting, while having a low variance and high bias tends to lead to underfitting.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/bullseye.png"><img alt="_images/bullseye.png" src="_images/bullseye.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Bias vs variance. <a class="reference external" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Source.</a></span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/biasvariance.png"><img alt="_images/biasvariance.png" src="_images/biasvariance.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">The bias-variance trade-off. On the left, the model underfits; on the right, it overfits. <a class="reference external" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Source.</a></span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/over_under_fitting.png"><img alt="_images/over_under_fitting.png" src="_images/over_under_fitting.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Over-, and under fitting for a one-dimensional regression problem. <a class="reference external" href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">Source.</a></span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/over_under_fitting_classification.jpg"><img alt="_images/over_under_fitting_classification.jpg" src="_images/over_under_fitting_classification.jpg" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Over-, and under fitting for a two-dimensional classification problem. <a class="reference external" href="https://datahacker.rs/018-pytorch-popular-techniques-to-prevent-the-overfitting-in-a-neural-networks/">Source.</a></span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recently people started to investigate new thinking frameworks about generalization and the bias-variance trade-off. These efforts are motivated by currently not understood observed phenomena in Deep Learning.</p>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">#</a></h2>
<section id="sgd">
<h3>SGD<a class="headerlink" href="#sgd" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A common question in DL is: If I increase my step size by a factor of <span class="math notranslate nohighlight">\(\alpha\)</span>, how should I change my batch size to get a similar behavor in my loss? Sometimes the answer is: You have to keep the variance of your stochastic gradient update constant to get the same result. Thanks to the limit theorem this means you get similar behavor if you keep <span class="math notranslate nohighlight">\(\frac{\text{step size}}{\sqrt{\text{batch size}}}\)</span> constant. If we use the theorem about the convergence of SGD and the central limit theorem, and try to answer the question in the limit in terms of the ball we converge into, we get the answer to keep <span class="math notranslate nohighlight">\(\frac{\text{step size}}{\text{batch size}}\)</span> constant. Derive both fractions.</p></li>
</ul>
<div class="toggle docutils container">
<p>The first fraction results from looking at the variance of the stochastic gradient update:</p>
<div class="math notranslate nohighlight">
\[\text{var}(t\frac{1}{\text{batch size}}\sum_i g_i) = \frac{t^2}{n}\sum_i\text{var}(g_i),\]</div>
<p>here <span class="math notranslate nohighlight">\(g_i\)</span> are stochastic gradients.</p>
<p>The second fraction we get if we look at</p>
<div class="math notranslate nohighlight">
\[\mathbb{E} f(\overline x_k) \le f(x^*) + \frac{\|x_0-x^*\|_2^2}{2tk} + \frac{t\sigma^2}{2}\]</div>
<p>and consider that central limit theorem tells us that <span class="math notranslate nohighlight">\(\sigma^2\propto \frac{1}{\text{batch size}}\)</span>.</p>
<p>This means that if we want to converge into a ball given by <span class="math notranslate nohighlight">\(\frac{t\sigma^2}{2}\)</span> of the same size, we have the scaling law that the size of the ball does not change if <span class="math notranslate nohighlight">\(\frac{\text{step size}}{\text{batch size}}\)</span> stays constant. This obviously only holds for <span class="math notranslate nohighlight">\(t L \le 1\)</span>.</p>
</div>
<ul class="simple">
<li><p>What advantage does SGD have over GD from a compute/memory point of view?</p></li>
<li><p>What advantage does SGD w.r.t. to instable stationary points.</p></li>
<li><p>Train simple MLPs on MNIST.</p>
<ol class="arabic simple">
<li><p>One for each of the following step size and batch size pairs: <span class="math notranslate nohighlight">\((1e-4, 2)\)</span>, <span class="math notranslate nohighlight">\((5e-4, 10)\)</span>, <span class="math notranslate nohighlight">\((1e-3, 20)\)</span>, <span class="math notranslate nohighlight">\((5e-4, 50)\)</span>, <span class="math notranslate nohighlight">\((1e-3, 200)\)</span>. Keep all other parameters constant.</p></li>
<li><p>Create of contraining the training loss curves of all the trained models.</p></li>
<li><p>Analyzte the two losses curve sets given by the relationships <span class="math notranslate nohighlight">\(\frac{\mbox{step size}}{\mbox{batch size}}=\mbox{const}\)</span> and <span class="math notranslate nohighlight">\(\frac{\mbox{step size}}{\sqrt{\mbox{batch size}}}=\mbox{const}\)</span>.</p></li>
</ol>
</li>
<li><p>Show the “number of iterations” part of the theorem.</p></li>
</ul>
<div class="toggle docutils container">
<p>We want a bound on <span class="math notranslate nohighlight">\(\epsilon/2\)</span>. We begin with</p>
<div class="math notranslate nohighlight">
\[\mathbb{E} f(\overline x_k) - f(x^*) \le \epsilon/2 \le \frac{\|x_0-x^*\|_2^2}{2tk} + t\sigma^2.\]</div>
<p>This gives</p>
<div class="math notranslate nohighlight">
\[t\epsilon \le \frac{\|x_0-x^*\|_2^2}{k} + 2t^2\sigma^2\]</div>
<p>and by plugging in <span class="math notranslate nohighlight">\(t=1/\sqrt{k}\)</span> we get</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\sqrt{k}}\epsilon \le \frac{\|x_0-x^*\|_2^2}{k} + \frac{2}{k}\sigma^2\]</div>
<div class="math notranslate nohighlight">
\[\sqrt{k} \le \frac{\|x_0-x^*\|_2^2 + 2\sigma^2}{\epsilon}.\]</div>
<p>Squaring both sides completes the proof.</p>
</div>
</section>
<section id="bias-variance">
<h3>Bias-variance<a class="headerlink" href="#bias-variance" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Name two things that can cause overfitting.</p></li>
</ul>
<div class="toggle docutils container">
<ol class="arabic simple">
<li><p>Too little trainig data.</p></li>
<li><p>A too large/complex model.</p></li>
</ol>
</div>
<ul class="simple">
<li><p>Complete the missing parts of the Bias-variance decomposition proof.</p></li>
<li><p>Would a linear regression under- or overfit on MNIST?</p></li>
<li><p>How can the split into a training, validation, and test set help to avoid overfitting?</p></li>
<li><p>Train an MLP on MNIST that overfits.</p></li>
<li><p>Train an MLP on MNIST that underfits.</p></li>
<li><p>When fitting a general polynomial of degree <span class="math notranslate nohighlight">\(p\)</span> to data, how does the degree of the polynomial influence over- and underfitting?</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="02_deep_vs_shallow.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Deep vs shallow</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="04_invariance_robustness.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Invariance &amp; robustness</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sgd-convergence-guarantees">
   SGD convergence guarantees
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-biasvariance-trade-off">
   The bias–variance trade-off
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sgd">
     SGD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias-variance">
     Bias-variance
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sören Dittmer
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>