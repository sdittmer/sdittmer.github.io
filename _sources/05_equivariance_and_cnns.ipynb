{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184d88f8-337b-41b0-bd95-b2ad6f0f1460",
   "metadata": {},
   "source": [
    "# Equivariance & convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbc74e-debb-4a8a-adef-96e55084b104",
   "metadata": {},
   "source": [
    "Last lecture we asked whether we can prevent the model from doing something if we know it shouldn't. This lecture we will try to explore the tool of equivariance to do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b193e-fdda-4cb1-97cb-762ceeb76282",
   "metadata": {},
   "source": [
    "* Together with robustness and invariance, we can use equivariance to get deep insides into many popular network architectures, e.g., Convolutional Networks and Graph Networks (including transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f2e4a-3abf-48ec-a609-8805be598922",
   "metadata": {},
   "source": [
    "## What is equivariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242a998-59a4-4539-829b-1ab4a9f49dc8",
   "metadata": {},
   "source": [
    "````{prf:definition} Equivariance\n",
    ":label: equivariance_def\n",
    "\n",
    "Let $G$ be a group and \n",
    "\n",
    "$$f:X \\to Y$$\n",
    "\n",
    "a mapping between vector spaces, e.g., a neural network.\n",
    "\n",
    "Further let $\\rho_X$ and $\\rho_Y$ be representations of $G$ on $X$ and $Y$.\n",
    "\n",
    "We call $f$ equivariant (w.r.t. $\\rho_X$ and $\\rho_Y$) if $f$ commutes with the group actions, i.e.,\n",
    "\n",
    "$$f \\circ \\rho_X(g) = \\rho_Y(g) \\circ f \\ \\forall g \\in G.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f2429-aeae-408f-8e1a-4d68aceed337",
   "metadata": {},
   "source": [
    "* I.e., the output changes under the tranformations given by the representations in the same way as the input.\n",
    "* E.g., if $G$ is the group of rotations, the output rotates if the input does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1902e7cf-924a-44f6-ba76-98e791577ed2",
   "metadata": {},
   "source": [
    "````{prf:lemma} Connecting Invariance & Equivariance\n",
    ":label: invariance_equivariance_lemma\n",
    "\n",
    "Let $G$ be a group and $f:X \\to \\mathbb{R}$ a mapping.\n",
    "Further let $\\rho$ be an orthogonal representations of $G$ on $X$.\n",
    "\n",
    "Then the gradient field $F=\\nabla f:X\\to X$ is equivariant.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212ac3d-d1de-4012-ab61-b7d43c363075",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "We have $f(\\rho(g)x) = f(x)$ and therefore also $F(x) = \\rho(g)^TF(\\rho(g)x)$.\n",
    "Since this implies ${\\rho(g)^T}^{'-1} F(x) = F(\\rho(g)x)$ we are done.\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb3dc9-b7bd-489f-81ce-8ec8e1c443b4",
   "metadata": {},
   "source": [
    "## How to get equivariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7611152-99f5-4688-8417-71c8376b9653",
   "metadata": {},
   "source": [
    "````{prf:lemma} Group averaging\n",
    ":label: group_averaging_lemma\n",
    "\n",
    "Let $G$ be a group and $f:X \\to Y$ a mapping between vector spaces.\n",
    "Further let $\\rho_X$ and $\\rho_Y$ be representations of $G$ on $X$ and $Y$.\n",
    "\n",
    "Then the mapping\n",
    "\n",
    "$$x \\mapsto \\frac{1}{|G|} \\sum_{g\\in G} \\rho_Y(g^{-1})f(\\rho_X(g)x)$$\n",
    "\n",
    "is called group averaging and is equivariant.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418753db-946e-49b3-9cf7-cb2fe1f676d7",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "We calculate:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " & \\frac{1}{|G|} \\sum_{g\\in G} \\rho_Y(g^{-1})f(\\rho_X(g)\\rho_X(h)x) \\\\\n",
    "=& \\frac{1}{|G|} \\sum_{g\\in G} \\rho_Y(g^{-1})f(\\rho_X(gh)x) \\\\\n",
    "=& \\frac{1}{|G|} \\sum_{a\\in G} \\rho_Y(ha^{-1})f(\\rho_X(a)x) \\\\\n",
    "=& \\rho_Y(h) \\frac{1}{|G|} \\sum_{a\\in G} \\rho_Y(a^{-1})f(\\rho_X(a)x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c229f-752d-4b1d-9b4d-b531c679629e",
   "metadata": {},
   "source": [
    "We will now investigate a more involved but often also more powerful approach to produce an equivariant linear mapping $\\mathcal{K}:X\\to Y$. We can then use this approach in a neural network layer $x\\mapsto \\phi(\\mathcal{K}x + b)$, that are equivariant -- assuming the bias $b$ and the activation function $\\phi$ are such that they do not break the equivariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce010ed-9b67-4ca7-91ac-89f1b98111ba",
   "metadata": {},
   "source": [
    "````{prf:definition} $G$-homogeneous\n",
    ":label: homogeneous_def\n",
    "\n",
    "Let $G$ be a group with a representation $\\rho$ on a topological space $X$. We call $X$ $G$-homogeneous if $G$ can act transitively on it.\n",
    "I.e., for any two $x, y\\in X$ $\\exists g\\in G$ such that $\\rho(g)x = y.$\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ef8fc-04a7-4a20-9909-c5401b18b8d2",
   "metadata": {},
   "source": [
    "````{prf:theorem} $G$-convolution is all you need ([Source](https://arxiv.org/pdf/1909.12057.pdf), [Source](https://arxiv.org/pdf/1802.03690.pdf))\n",
    ":label: group_conv_theorem\n",
    "Let\n",
    "\n",
    "$$\\mathcal{K}:L_2(X) \\ni f \\mapsto \\int_X \\tilde k(x, y)f(x) dx \\in L_2(Y)$$\n",
    "\n",
    "be an integral operator between signals on $G$-homogeneous spaces $X$ and $Y$. Let also $e\\in Y$ be some \"origin element\" and $\\rho_Y(g_y)$ an orthonormal representation of $g_y\\in G$ such that $\\forall y\\in Y$ we have $y=\\rho_Y(g_y)e$.\n",
    "\n",
    "Then $\\mathcal{K}$ is $G$-equivariant if(f)\n",
    "\n",
    "$$[\\mathcal{K}f](y) = \\int_X k(\\rho_X(g_y^{-1})x) f(x) dx =: (k \\ *_G f)(y)$$\n",
    "\n",
    "where $k:X\\to \\mathbb{R}$ the **kernel** and $\\rho_X$ a representation on $X$.\n",
    "\n",
    "This is kind of map is called a **group convolution**.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83f8ad-e60a-4801-9c1e-6b46d0b59ede",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "We will only show the \"if\" not the \"iff\" as that direction is more involved and typically uses different versions of Schur’s lemma. I.e., we show that $(k\\ *_G f)(y)$ is $G$-equivariant.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " & \\int_X k\\left(\\rho_X(g_{y})^{-1}x\\right) f(\\rho_X(g)x) dx \\\\\n",
    "=& \\int_X k\\left(\\rho_X(g_{y})^{-1} \\rho_X(g)x\\right) f(\\rho_X(g)\\rho_X(g^{-1})x) |\\det \\rho_X(g^{-1})| dx \\\\\n",
    "=&  \\int_X k\\left(\\rho_X([gg_{y}]^{-1}) x\\right) f(x) dx\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd838b-bf0e-4640-9bb1-ad20d0bd107f",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For the algebra lovers: If we consider the subgroup $H$ given by $Y=G/H$ such that $H=\\mbox{Stabilizer}_G(e)$. Then the whole subgroup leaves $e$ unchanged and we have\n",
    "\n",
    "$$k(x) = \\frac{1}{|\\det \\rho_X(h)|} k(\\rho_X(h^{-1})x) \\ \\forall h\\in H.$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e242279-0cb3-4517-8c65-a34335ba6b3a",
   "metadata": {},
   "source": [
    "## Shift equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502ced7-ddd5-4c0d-99f4-f2e2f46188e8",
   "metadata": {},
   "source": [
    "Assume that you want a network that works on an image processing task -- e.g., denoise -- to also work on images where the content is shifted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98982faa-22bd-4b6d-9072-a186f397c99b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "plt.xkcd()\n",
    "\n",
    "\n",
    "def plot(center_parameter=2):\n",
    "    f = center_parameter\n",
    "\n",
    "    img = np.zeros((512, 512))\n",
    "    img[512//f-32 : 512//f+32, 512//f-32 : 512//f+32] = 1.\n",
    "\n",
    "    img_noisy = np.clip(img + np.random.randn(*img.shape), 0, 1)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"noisy\")\n",
    "    plt.imshow(img_noisy)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"clean\")\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot(center_parameter=2)\n",
    "plot(center_parameter=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce9946-2ee6-4c96-a875-abba5454e7f3",
   "metadata": {},
   "source": [
    "This means we want our neural network to be shift equivariant. If we define a shift via the mapping\n",
    "\n",
    "$$\\rho(y)f(x) = f(x - y)$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$(k \\ *_G f)(y) = \\int_X k(y - x) f(x) dx =: (k * f)(y).$$\n",
    "\n",
    "This is the most popular group convolution which generally simply called convolution is denoted by \"$*$\" without a subscript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99dbfa1-1488-4911-b562-c0ad8857c837",
   "metadata": {},
   "source": [
    "So if we want to create a shift equivariant neural networks, it makes sense to use convolutions. This leads us to Convolutional Neural Networks (CNNS), usually used on data with spacial structure (i.e., shifts make sense), most prominently images. CNNs are the result of restricting the linear parts of each layer\n",
    "\n",
    "$$x\\mapsto \\mathcal{K}x + b$$\n",
    "\n",
    "to be shift equivariant via using convolution. This has lead to the following definition of a CNN layer.\n",
    "\n",
    "We define a CNN layer as the mapping\n",
    "\n",
    "$$\\mathbb{R}^{c_I \\times m \\times n}\\ni (x_i)_{i=1}^{c_I} \\mapsto K x := \\left(b_j + \\sum_{i=1}^{c_I} k_{i,j} * x_i\\right)_{j=1}^{c_O} \\in \\mathbb{R}^{c_O\\times \\tilde m \\times \\tilde n}$$\n",
    "\n",
    "where $c_I$ and $c_O$ are the socalled the number of **input channels** and **output channels**. $m$ and $n$ are the **height** and **width** of the input image and $\\tilde m$ and $\\tilde n$ are the of the input image. Further $b_j\\in\\mathbb{R}$.\n",
    "\n",
    "```{note}\n",
    "In the finite & discrete setting as above, the convolution operator runs into the problem that it has to deal with boundaries and, therefore, boundary effects. There are several ways one can deal with them. For simplicity, we will always assume that we deal with them using zero-padding. I.e., we assume that the out-of-range values are $0$ and lead to $m = \\tilde m$ and $n = \\tilde n$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6c50b-c066-4857-acdb-3940a829070b",
   "metadata": {},
   "source": [
    "```{note}\n",
    "In most deep learning frameworks, convolutional layers do not use convolutions but the cross-correlations. But since the cross-correlation is defined as\n",
    "\n",
    "$$\\int_X k(x + y) f(x) dx,$$\n",
    "\n",
    "it is equivalent to convolution if one flips the kernel. Therefore it is irrelevant whether one uses convolutions or cross-correlations unless one works manually with the kernel, which in practice, one rarely does.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc1a11-d069-48c1-b3b3-7265f70f1cde",
   "metadata": {},
   "source": [
    "```{note}\n",
    "One tends to avoids asymmetries caused by discretization by using kernels with an odd (non-even) width.\n",
    "\n",
    "```{figure} images/conv_and_padding.gif\n",
    "---\n",
    "height: 300px\n",
    "---\n",
    "2d-convolution with zero-padding. [Source.](https://medium.com/@draj0718/zero-padding-in-convolutional-neural-networks-bf1410438e99)\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca198fd-b423-4c52-a8ad-2ee2bb6ef24b",
   "metadata": {},
   "source": [
    "## Shift robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a088393-931e-48ee-9c3e-7002aae5ce67",
   "metadata": {},
   "source": [
    "### Two ways to get shift invariance (ignoring boundary effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93f84a-6ae2-4b68-8d0d-ce55b5f38e86",
   "metadata": {},
   "source": [
    "* Goal: Be robust against shifts.\n",
    "* It is often easier to think about invariance than robustness first.\n",
    "* Up to boundary effect, these are simple functions invariant to shifts:\n",
    "    + Compute the maximal (pixel) value of the image.\n",
    "    + Compute the mean of the (pixel) values of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306aac0-fa53-46ab-8946-f04158d13ccf",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "The goal of pooling is to introduce shift robustness. It uses the same trick that we discussed above for invariance, but applies it to image patches.\n",
    "\n",
    "```{figure} images/max_pooling.png\n",
    "---\n",
    "height: 300px\n",
    "---\n",
    "Max pooling example. [Source.](https://programmathically.com/what-is-pooling-in-a-convolutional-neural-network-cnn-pooling-layers-explained/)\n",
    "```\n",
    "\n",
    "```{figure} images/mean_pooling.png\n",
    "---\n",
    "height: 300px\n",
    "---\n",
    "Mean pooling example. [Source.](https://programmathically.com/what-is-pooling-in-a-convolutional-neural-network-cnn-pooling-layers-explained/)\n",
    "```\n",
    "\n",
    "Pooling is applied channelwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42703949-e0fd-4f02-bd1b-3527b5ffb70e",
   "metadata": {},
   "source": [
    "## An example CNN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5edc73-cc4e-46e8-b08d-b903f5289aaf",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546f03d-c1bc-4d4a-86f2-42682627966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = MNIST(\n",
    "    root = 'datasets',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    ")\n",
    "\n",
    "test_data = MNIST(\n",
    "    root = 'datasets',\n",
    "    train = False,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45c059-2627-4060-a00b-6e80d7ef2344",
   "metadata": {},
   "source": [
    "### Print some information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2d699-7056-4473-9e45-dacdf9752392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape: {tuple(train_data.data.shape)}\")\n",
    "print(f\"Test data shape: {tuple(test_data.data.shape)}\")\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(f\"A training sample with the label {train_data.targets[0]}.\")\n",
    "plt.imshow(train_data.data[0], cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Histogram of training labels.\")\n",
    "plt.hist(np.array(train_data.targets), bins=10)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Histogram of test labels.\")\n",
    "plt.hist(np.array(test_data.targets), bins=10)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88bd08-8ea2-47e7-ad71-37f52e632f08",
   "metadata": {},
   "source": [
    "### Setup data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43e3de-a7a4-4297-918b-2fea5b0d87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84477746-c4fe-43b5-8112-f8a58e31dcc0",
   "metadata": {},
   "source": [
    "### Setup simple CNN and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46352a75-263e-4488-94a5-4dea53cb5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(512, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.relu(self.pool(self.conv0(x)))\n",
    "        z = self.relu(self.pool(self.conv1(z)))\n",
    "        z = self.relu(self.pool(self.conv2(z)))\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.dense(z)\n",
    "        y = self.softmax(z)\n",
    "\n",
    "        assert y.shape == (len(x), 10), f\"{y.shape=}\"\n",
    "        assert torch.allclose(y.sum(dim=-1), torch.ones(1)), \"Softmax did not normalize correctly.\"\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742df60-2238-44a9-985f-6dcc60916631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bdd41-92cd-43fd-a718-cdfba34061df",
   "metadata": {},
   "source": [
    "### Run training and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c78c5-5874-42cc-a0d1-413b0b19ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193711e-22ad-4623-9506-ccafa6d238cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_losses_epoch = []\n",
    "    test_losses_epoch = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for images, labels in data_loader_train:\n",
    "        assert images.shape == (len(images), 1, 28, 28), f\"{images.shape=}\"\n",
    "        assert labels.shape == (len(images),), f\"{images.shape=}, {labels.shape=}\"\n",
    "        assert torch.all(0 <= images) and torch.all(images <= 1)\n",
    "\n",
    "        preditions = model(images)\n",
    "        loss = criterion(preditions, labels)\n",
    "        train_losses_epoch.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in data_loader_test:\n",
    "        preditions = model(images)\n",
    "        loss = criterion(preditions, labels)\n",
    "        test_losses_epoch.append(loss.item())\n",
    "\n",
    "    test_losses.append(np.mean(test_losses_epoch))\n",
    "\n",
    "    print(f\"Epoch {epoch}'s train and test loss: {train_losses[-1]:.3f}, {test_losses[-1]:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeadb16-14ee-4724-8ba4-f03d41eb69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"training\")\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"mean BCE loss\")\n",
    "plt.gcf().set_size_inches(10, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250dcae1-3714-4b3f-84fc-d31a898b8caf",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "* Try to find an MLP with maximal accuracy on MNIST. Try to find a CNN with maximal accuracy on MNIST.\n",
    "* Think about why in the training above the training loss is higher then the test loss? Hint: the code is \"problematic\" if you only train for a few epochs -- like we did.\n",
    "* Think about why aliasing could be a problem for pooling in the context of equivariance (even ignoring boundary effects)? Hint: Read about the Nyquist–Shannon sampling theorem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a684eb7-7c9a-4429-994c-d41f05bec667",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
