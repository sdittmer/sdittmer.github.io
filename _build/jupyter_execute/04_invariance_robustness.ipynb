{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81adb584-094e-4356-be52-ce712a3f9384",
   "metadata": {},
   "source": [
    "# Invariance & robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839b386-a688-4152-b858-ea07926008bd",
   "metadata": {},
   "source": [
    "We will try to answer: We know we can approximate anything, but we also know about the bias-variance trade-off. Can we prevent the model from doing something if we know it shouldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d55ec1-69b6-4af0-9c09-de7ef55cc720",
   "metadata": {},
   "source": [
    "For this we will think about how we can enforce or encourage robustness and in the limit invariance of the functions learned by neural networks.\n",
    "\n",
    "We begin by defining what it means for a function to be invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf0eb5-cd86-4df9-a07f-7d9b0ae48649",
   "metadata": {},
   "source": [
    "````{prf:definition}\n",
    ":label: invariance_def\n",
    "\n",
    "Let $G$ be a set of transformations $\\rho: X\\to X$. We call a function\n",
    "\n",
    "$$f: X \\to Y$$\n",
    "\n",
    "invariant under $G$ if\n",
    "\n",
    "\n",
    "$$f = f \\circ \\rho \\ \\forall \\rho \\in G.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4bad0-500a-45dd-8cec-c3716722ba90",
   "metadata": {},
   "source": [
    "````{prf:definition}\n",
    ":label: robust_def\n",
    "\n",
    "Let $G$ be a set of transformations $\\rho: X\\to X$. We call a function\n",
    "\n",
    "$$f: X \\to Y$$\n",
    "\n",
    "$\\frac{1}{L}$-robust under $G$ if\n",
    "\n",
    "\n",
    "$$\\|f(x) - f(\\rho(x))\\| \\le L \\|x - \\rho(x)\\| \\ \\forall \\rho \\in G\\ \\wedge \\ \\forall x\\in X.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155152b-890e-4cdf-a68a-8a092a744139",
   "metadata": {},
   "source": [
    "* This is one of many possible robustness definitions.\n",
    "* In some cases you might want to generalize {prf:ref}`robust_def` to involve more specific distance measures than norms.\n",
    "* $\\infty$-robustness is invariance.\n",
    "* There are three main ways try to introduce invariance or robustness into a network.\n",
    "    1. Modfiy the inputs during training.\n",
    "    2. Modify the loss function.\n",
    "    3. Modify the architecture.\n",
    "        - The only way to enforce not just encourage invariance or robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c53df7-08d4-4ceb-b926-5ce2b9dddb0f",
   "metadata": {},
   "source": [
    "````{prf:definition}\n",
    ":label: lipschitz_def\n",
    "\n",
    "We call a function\n",
    "\n",
    "$$f: X \\to Y$$\n",
    "\n",
    "$L$-Lipschitz continuous if\n",
    "\n",
    "$$\\|f(x) - f(x')\\| \\le L \\|x - x'\\| \\ \\forall x, x'\\in X.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a85d5-44eb-4790-b3b6-e13bb213ee2b",
   "metadata": {},
   "source": [
    "* This means that the Lipschitz property provides a \"general type\" of robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8efb5-3748-4941-b7d1-4a753cf0b37d",
   "metadata": {},
   "source": [
    "````{prf:lemma}\n",
    ":label: lipschitz_robust_lemma\n",
    "\n",
    "Let $f: X \\to Y$ be $\\frac{1}{L}$-robust under a transformation set $G$. If $G$ acts transitively on $X$ then $f$ is L-Lipschitz.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bddeb8-aac1-4c4e-9190-d104504536b3",
   "metadata": {},
   "source": [
    "````{prf:lemma}\n",
    ":label: lipschitz_grad_lemma\n",
    "\n",
    "If $f:\\mathbb{R}^n\\to \\mathbb{R}^m$ is differentiable, then, if\n",
    "\n",
    "$$L = \\sup_{x\\in X} \\|Df(x)\\|$$\n",
    "\n",
    "exists, $f$ is $L$-Lipschitz. Here $Df(x)$ denotes the derivative of $f$ w.r.t. $x$.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e3184-b9c5-486f-83a1-603286dd1298",
   "metadata": {},
   "source": [
    "````{prf:lemma}\n",
    ":label: lipschitz_concat_lemma\n",
    "\n",
    "If $f:Z\\to Y$ is $L_f$-Lipschitz and $g:X\\to Z$ is $L_g$-Lipschitz, then $f \\circ g$ is $L_fL_g$-Lipschitz.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba475da9-dbe8-42ca-ad7b-aba82d85e748",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "We calculate\n",
    "\n",
    "$$\\|f(g(x)) - f(g(x'))\\| \\le L_f \\|g(x) - g(x')\\| \\le L_fL_g \\|x - x'\\|.$$\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc6b7d-ccb3-4fd2-99b4-fb05c3cc0c5c",
   "metadata": {},
   "source": [
    "````{prf:lemma}\n",
    ":label: mlp_lipschitz_lemma\n",
    "\n",
    "Every MLP with a Lipschitz activation function is Lipschitz continuous.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185251b-ccdc-4298-9893-6778eba682c4",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "\n",
    "As an MLP has a finite number of layers and thanks to {prf:ref}`lipschitz_concat_lemma` we only need to prove that each single layer is Lipschitz.\n",
    "\n",
    "As each layer is of the form\n",
    "\n",
    "$$\\phi(Ax + b)$$\n",
    "\n",
    "where $\\phi$ is the activation function, $A$ a matrix, and $b$ a vector. Again, since we know that $\\phi$ is Lipschitz, we can use {prf:ref}`lipschitz_concat_lemma` to reduce the problem to showing that\n",
    "\n",
    "$$Ax + b$$\n",
    "\n",
    "is Lipschitz. We calculate\n",
    "\n",
    "$$\\|(Ax + b) - (Ax' + b)\\| = \\|A(x-x')\\| \\le \\|A\\| \\|x-x'\\|.$$\n",
    "\n",
    "As $A$ is a matrix and thereby a bounded linear operator $\\|A\\|$ exists and we are done.\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f01432-4294-48ef-8771-8ea54e1f9c42",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Most activation functions are Lipschitz continuous. E.g., $\\mbox{ReLU}, \\mbox{sigmoid}, \\tanh.$\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "While in theory, all MLPs are Lipschitz continuous, in practice, this can be irrelevant as the Lipschitz constant is often uncomfortably large.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258264b-895d-4539-8241-505ed171ebea",
   "metadata": {},
   "source": [
    "## Tricks to make a network invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ceaa8e-e678-4a4a-8524-03faed82edb1",
   "metadata": {},
   "source": [
    "````{prf:definition} PointNet\n",
    ":label: PointNet_def\n",
    "\n",
    "Let $f:X \\to Z$ and $g:Z\\to Y$ be neural networks. We then define a **PointNet** as\n",
    "\n",
    "$$\\tilde X \\mapsto g\\left(\\sum_{x\\in\\tilde X} f(x)\\right).$$\n",
    "\n",
    "Here $\\tilde X\\subset X$ is a set of input points.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be80e0b-055c-46b4-b568-c21f45b6ceab",
   "metadata": {},
   "source": [
    "* A special property of the PointNet is that it can take in abitrarily many vectors as one joint input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686ef1d-597e-4d15-9293-0d75dcb5f9cd",
   "metadata": {},
   "source": [
    "````{prf:lemma}\n",
    ":label: PointNet_lemma\n",
    "\n",
    "PointNets are permutation invariant w.r.t. the elements in $\\tilde X$.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2951730-3392-42a9-8e70-8417b684314e",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "\n",
    "As the sum $\\sum_{x\\in\\tilde X} f(x)$ is commutative, i.e., permutation invariant we are done.\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1ce7d-7b20-4fa0-80be-abb9181f0f2e",
   "metadata": {},
   "source": [
    "A recap of what a group and a group representation is.\n",
    "\n",
    "````{prf:definition} Group\n",
    ":label: group_def\n",
    "\n",
    "A **group** is a set $G$ together with a binary operation\n",
    "\n",
    "$$\\cdot:G \\times G\\ni (a, b) \\mapsto a \\cdot b \\in G,$$\n",
    "\n",
    "s.t. we have\n",
    "* associativity, i.e., $(a\\cdot b)\\cdot c = a\\cdot (b\\cdot c)$\n",
    "* an identity element, i.e., $\\exists e \\in G$ s.t., $e\\cdot a = a =  a \\cdot e \\ \\forall a\\in G$,\n",
    "* and an inverse, i.e., for each $a \\in G\\ \\exists\\ b\\in G$ s.t. $a\\cdot b=b\\cdot a = e$.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35c322-fa98-4b88-8894-e000ebae5e17",
   "metadata": {},
   "source": [
    "````{prf:theorem} Cayley's theorem\n",
    ":label: cayley_theorem\n",
    "\n",
    "Every group $G$ is isomorphic to a subgroup of a symmetric group (all the bijections from the set to itself).\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9b9a6-a072-4649-9a0b-672b49bd3be4",
   "metadata": {},
   "source": [
    "````{prf:definition} Group representation\n",
    ":label: representation_def\n",
    "\n",
    "Let $G$ be a group and $V$ a vector space. A **representation** of $G$ on $V$ is a group homomorphism from $G$ to $\\mbox{GL}(V)$. I.e., a map\n",
    "\n",
    "$$\\rho: G \\to \\mbox{GL}(V)$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$\\rho(a\\cdot b) = \\rho(a)\\rho(b) \\ \\forall a,b\\in G.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee40718-42e5-4dbc-8f14-7f65bfbbc869",
   "metadata": {},
   "source": [
    "We can use the trick of the PointNet to create a network that is invariant to the transformations from a group $G$ with a representation $\\rho$.\n",
    "\n",
    "For two networks $f:X\\to Z$ and $g:Z\\to Y$ we can simply define the network\n",
    "\n",
    "$$x \\mapsto g\\left(\\sum_{a\\in G} f(\\rho(a)x)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a2b31-13db-40c2-a7d0-abe75b1def5c",
   "metadata": {},
   "source": [
    "## Tricks to make a network robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d978693-d0c2-4775-85bb-ebeda9b59cac",
   "metadata": {},
   "source": [
    "### Robustness gradient penalty training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93a76d-ce75-4c72-aa64-c4e34493244c",
   "metadata": {},
   "source": [
    "Usually we train a network $f$ with a loss of the shape\n",
    "\n",
    "$$L(\\Theta, \\mathscr{D}_\\text{train}) = \\mathbb{E}_{(x,y)} \\mathscr{l}(f_\\Theta(x), y).$$\n",
    "\n",
    "A simple way to improve the robustness of the trained network ([source](https://arxiv.org/pdf/1711.09404.pdf)) is to augment the loss via {prf:ref}`lipschitz_grad_lemma`:\n",
    "\n",
    "$$L(\\Theta, \\mathscr{D}_\\text{train}) = \\mathbb{E}_{(x,y)} \\mathscr{l}(f_\\Theta(x), y) + \\lambda \\|\\nabla_x f_\\Theta(x)\\|_p^p.$$\n",
    "\n",
    "Usually one choses $p=2$ and $\\lambda \\in [1, 10]$.\n",
    "\n",
    "* Only easy and cheap if $f_\\Theta(x)$ is a scalar. See [here](https://arxiv.org/pdf/2012.13247.pdf) for non-scalars.\n",
    "* This method only tries to minimize a lower bound on the Lipschitz constant, so there is no guarantee that this function will be robust. Indeed, if you explicitly look for so-called advrserial examples, you will find them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f62370-c984-4e8c-bfa9-d620cf06a488",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62deae8-ae54-4a9a-bf49-b0c9473a0bf0",
   "metadata": {},
   "source": [
    "#### What is the goal?\n",
    "* As we discussed, Lipschitz continuity provides a general, unspecific type of robustness.\n",
    "* We can achieve a more specific type of robustness via data augmentation.\n",
    "* Again, we assume a set of transformations/augmentations $G$ and neural network $f:X\\to Y$ and we want $f$ to be robust under $G$, i.e., \n",
    "\n",
    "$$\\|f(x) - f(\\rho(x))\\| \\le L \\|x - \\rho(x)\\| \\ \\forall \\rho \\in G\\ \\wedge \\ \\forall x\\in X.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31812d9b-ffaf-4db4-82ff-f17e3ae01d95",
   "metadata": {},
   "source": [
    "#### The method\n",
    "* Data agumentation tries to encourage this robustness w.r.t. $G$ by defining a probability distribution $\\mathcal{G}$ over $G$ with a probability density function $p:G\\to\\mathbb{R}_{\\ge0}$. This distibution of transformations is then used to define the loss\n",
    "\n",
    "$$L(\\Theta, \\mathscr{D}_\\text{train}) = \\mathbb{E}_{(x,y)} \\mathbb{E}_{\\rho\\sim\\mathcal{G}} \\mathscr{l}(f_\\Theta(\\rho(x)), y).$$\n",
    "\n",
    "* In practice, one does not compute the full expectation $\\mathbb{E}_{\\rho\\sim\\mathcal{G}}$ one only samples a few, usually even only one $\\rho$, to approximate the expectation. Using this simplification, we can reformulate the above setting via the probabilistic mapping $\\mathcal{A}:X\\ni x \\to \\rho(x)\\in X \\text{ with } \\rho\\sim\\mathcal{G}$ as\n",
    "\n",
    "$$L(\\Theta, \\mathscr{D}_\\text{train}) = \\mathbb{E}_{(x,y)} \\mathscr{l}(f_\\Theta(\\mathcal{A}(x)), y).$$\n",
    "\n",
    "```{note}\n",
    "In practice it might makes sense to make $G$ and $\\mathcal{G}$ data dependent, i.e., $G=(x, y)$ and $\\mathcal{G}=\\mathcal{G}(x, y)$; or alternativly $\\mathcal{A}(x, y)$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2b2d5-d9f8-47fe-8dec-a0babaaf3845",
   "metadata": {},
   "source": [
    "#### Which augmentations?\n",
    "* The core questions when using augmentations is: how to chose $G$ and $\\mathcal{G}$?\n",
    "* Ideally we should probably chose $\\mathcal{G}$ as\n",
    "\n",
    "$$\\arg\\min_\\mathcal{G} \\mathbb{E}_{(x,y)\\sim p_{x,y}} \\mathcal{l}(f^*(x), y) \\text{ where}$$\n",
    "\n",
    "$$f^* = \\arg\\min_f \\frac{1}{|\\mathcal{D}_\\text{train}|} \\sum_{(x,y)\\in \\mathcal{D}_\\text{train}} \\mathbb{E}_{\\rho\\sim\\mathcal{G}} \\mathcal{l}(f(\\rho(x)), y).$$\n",
    "* Problem: $\\nexists$ practical access to $p_{x,y}$\n",
    "\n",
    "* One way we could tackle that problem is by trying to come up with an $\\mathcal{A}$ such that the distribution arising from the mapping $\\mathcal{D}_\\text{train} \\sim (x, y) \\mapsto (\\mathcal{A}(x), y)$ approximates the true distribution $p_{x,y}$ as well as possible.\n",
    "    + This is also very hard.\n",
    "* Approach: Via negativa. Instead of approximating $p_{x,y}$, try to find augmentations $\\mathcal{A}$ that do not change the class of $y$. I.e., we use invariance/robustness.\n",
    "* Examples of good augmentations for image classification, e.g., cat vs dog:\n",
    "    + Adding some noise to the image.\n",
    "    + Rotate the image.\n",
    "    + Mirror the image.\n",
    "    + Slightly warp/deform the image.\n",
    "    + Crop the the image.\n",
    "    + Change brightness.\n",
    "    + Change contrast.\n",
    "    + Color distortion.\n",
    "    + Somehow combine two images of the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220b809-50b1-451e-afe4-8ab5019604fa",
   "metadata": {},
   "source": [
    "```{figure} images/augmentation_car.png\n",
    "---\n",
    "height: 200px\n",
    "---\n",
    "Some augmentations of a car. [Source.](https://viso.ai/computer-vision/image-data-augmentation-for-computer-vision/)\n",
    "```\n",
    "\n",
    "```{figure} images/augmentation_grid.jpg\n",
    "---\n",
    "height: 500px\n",
    "---\n",
    "Some strong augmentations of a Quokka. [Source.](https://github.com/aleju/imgaug)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a906ae-69f3-44ce-9556-9f915ace89c6",
   "metadata": {},
   "source": [
    "## When do you want which robustness or invariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd43e3d-870c-406d-a984-dbc32e7e72e8",
   "metadata": {},
   "source": [
    "* Rotation and mirror invariance for the input being an photo:\n",
    "    + Good idea for:\n",
    "        - Is there a cat in the image?\n",
    "    + Bad idea for:\n",
    "        - In which direction is the cat looking?\n",
    "* Permutation invariance of the input is a sequence of digits:\n",
    "    + Good idea for:\n",
    "        - What is the digit sum?\n",
    "    + Bad idea for:\n",
    "        - What is the square of the number which is by the sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e90fa-c4a2-4b52-949e-eee38eafb4e8",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Prove {prf:ref}`lipschitz_robust_lemma`\n",
    "* Prove {prf:ref}`lipschitz_grad_lemma`\n",
    "* Train a PointNet that can compute the digit sum of 1, 2, and 3 MNIST digits.\n",
    "    + Evaluate it also for 4 and 5 digits.\n",
    "* Use the Fashion MNIST dataset and introduce random 90°, 180°, 270° rotations. Then train an MLP and a rotation invariant version of that MLP on it. Compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6d3e8-5fd3-4f2e-9f6e-c091b07a6f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}