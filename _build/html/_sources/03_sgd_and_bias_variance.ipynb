{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f02fc3-5b1b-4fd2-84f6-0b6bc5f86a7e",
   "metadata": {},
   "source": [
    "# SGD guarantees & the bias–variance trade-off\n",
    "\n",
    "We will try to answer two:\n",
    "1. Can we understand what the training process (SGD) does in a simplified setting? \n",
    "2. How to pick a model considering its expected error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3b02d-3718-4fdb-805f-ee5d8b44a18b",
   "metadata": {},
   "source": [
    "## SGD convergence guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aba171-c416-4edd-9031-324f0b63be9c",
   "metadata": {},
   "source": [
    "````{prf:definition}\n",
    ":label: convexity_def\n",
    "We call a function $f:\\mathbb{R}^n \\to \\mathbb{R}$ convex if for all $t\\in[0,1]$\n",
    "\n",
    "$$f(tx + (1-t)y) \\le t f(x) + (1-t)f(y) \\ \\forall x, y.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e284e6-9a80-4f9f-abeb-8a75c9cbccde",
   "metadata": {},
   "source": [
    "````{prf:lemma}\n",
    ":label: convex_lemma\n",
    "\n",
    "If $f:\\mathbb{R}^n\\to\\mathbb{R}$ is differnetiable, it is convex iff\n",
    "\n",
    "$$f(x) - f(y) \\le \\langle \\nabla f(x), x-y\\rangle \\forall x, y.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1962d9-6eec-4691-8948-5d367c180eef",
   "metadata": {},
   "source": [
    "````{prf:lemma} A bound on functions with Lipschitz gradient\n",
    ":label: blg_lemma\n",
    "\n",
    "If $f:\\mathbb{R}^n\\to\\mathbb{R}$ has an L-Lipschitz gradient, we have\n",
    "\n",
    "$$f(y) \\le f(x) + \\langle \\nabla f(x), y-x\\rangle + \\frac{L}{2}\\|y-x\\|_2^2 \\; \\forall x, y.$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97112ac5-0162-4c6a-9876-8dd2fe6c623d",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "Sketch:\n",
    "\n",
    "1. Write out the first/second order Taylor expansion.\n",
    "2. Use the Taylor inequality.\n",
    "3. Bound the Taylor inequality using $L$.\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5099a-9c7a-44cb-afcf-5877a77f1fc4",
   "metadata": {},
   "source": [
    "````{prf:theorem} Convergence of SGD\n",
    ":label: sgd_thm\n",
    "\n",
    "We assume that $f:\\mathbb{R}^n\\to\\mathbb{R}$ has an L-Lipschitz gradient, and is convex. We also assume that we use stochastic gradients $v_k$ from an unbiased estimator with a bounded variance\n",
    "\n",
    "$$\\mbox{Var}(v_k) = \\mathbb{E}\\|v_k\\|_2^2 - \\|\\mathbb{E}v_k\\|_2^2 \\le \\sigma^2.$$\n",
    "\n",
    "Then, for any SGD iteration $k$ with step size $t_k=t\\le \\frac{1}{L}$ we have\n",
    "\n",
    "$$\\mathbb{E} f(\\overline x_k) \\le f(x^*) + \\frac{\\|x_0-x^*\\|_2^2}{2tk} + t\\sigma^2,$$\n",
    "\n",
    "where $\\overline x_k = \\frac{1}{k} \\sum_{i=1}^{k}x_{i}$ and $x^* = \\arg\\min_x f(x)$.\n",
    "\n",
    "Further, we can *expect* to find an $\\epsilon/2$ optimal value (in terms of its loss) within\n",
    "\n",
    "$$k = \\frac{(2\\sigma^2 + \\|x_0-x^*\\|_2^2)^2}{\\epsilon^2}$$\n",
    "\n",
    "iterations, by setting $t = \\frac{1}{\\sqrt{k}}$. Here we assume $\\frac{1}{\\sqrt{k}} \\le \\frac{1}{L}$.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cae61e-5494-4396-a85d-5e726f547c51",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "The lemma above gives us\n",
    "\n",
    "$$f(x_{i+1}) \\le f(x_i) + \\langle\\nabla f(x_i), x_{i+1} - x_i\\rangle + \\frac{L}{2}\\|x_{i+1} - x_i\\|_2^2$$\n",
    "\n",
    "$$= f(x_i) - t\\langle\\nabla f(x_i),v_i\\rangle + \\frac{Lt^2}{2}\\|v_i\\|_2^2.$$\n",
    "\n",
    "Taking the expectation and using the bound on $t$ gives us\n",
    "\n",
    "$$\\mathbb{E}f(x_{i+1}) \\le f(x_i) - t \\|\\nabla f(x_i)\\|_2^2 + \\frac{Lt^2}{2} [ \\|\\nabla f(x_i)\\|_2^2 + \\mbox{Var}(v_i)]$$\n",
    "\n",
    "$$\\le f(x_i) - t (1-\\frac{Lt}{2})\\|\\nabla f(x_i)\\|_2^2 + \\frac{Lt^2}{2}\\sigma^2$$\n",
    "\n",
    "$$\\le f(x_i) - \\frac{t}{2} \\|\\nabla f(x_i)\\|_2^2 + \\frac{t}{2}\\sigma^2.$$\n",
    "\n",
    "Via convexity we know\n",
    "\n",
    "$$f(x^*) + \\langle\\nabla f(x_i), x_i - x^*\\rangle \\ge f(x_i)$$\n",
    "\n",
    "and therefore we have\n",
    "\n",
    "$$\\mathbb{E}f(x_{i+1}) \\le f(x^*) + \\langle\\nabla f(x_i), x_i - x^*\\rangle - \\frac{t}{2} \\|\\nabla f(x_i)\\|_2^2 + \\frac{t}{2}\\sigma^2.$$\n",
    "\n",
    "Since we assume an unbiased estimator this can be written as\n",
    "\n",
    "$$\\mathbb{E}f(x_{i+1}) \\le f(x^*) + \\langle\\mathbb{E} v_i, x_i - x^*\\rangle - \\frac{t}{2} \\|\\mathbb{E} v_i\\|_2^2 + \\frac{t}{2}\\sigma^2$$\n",
    "\n",
    "$$\\mathbb{E}f(x_{i+1}) \\le f(x^*) + \\langle\\mathbb{E} v_i, x_i - x^*\\rangle - \\frac{t}{2} (\\mathbb{E}\\|v_k\\|_2^2 - \\text{Var}(v_k)) + \\frac{t}{2}\\sigma^2$$\n",
    "\n",
    "$$= f(x^*) + \\mathbb{E} \\left[\\langle v_i, x_i - x^*\\rangle - \\frac{t}{2} \\| v_i\\|_2^2\\right] + t\\sigma^2$$\n",
    "\n",
    "$$= f(x^*) + \\mathbb{E} \\left[\\frac{1}{2t}\\left(\\|x_i-x^*\\|_2^2 - \\|x_i-x^*-tv_i\\|_2^2\\right)\\right] + t\\sigma^2$$\n",
    "\n",
    "$$= f(x^*) + \\mathbb{E} \\left[\\frac{1}{2t}\\left(\\|x_i-x^*\\|_2^2 - \\|x_{i+1}-x^*\\|_2^2\\right)\\right] + t\\sigma^2.$$\n",
    "\n",
    "By taking a sum and noticing the canceling terms we have\n",
    "\n",
    "$$\\sum_{i=0}^{k-1} \\mathbb{E} f(x_{i+1}) - f(x^*) \\le \\frac{1}{2t} \\left(\\|x_0 - x^*\\|_2^2 - \\mathbb{E}\\|x_k-x^*\\|_2^2\\right) + kt\\sigma^2$$\n",
    "\n",
    "$$\\le \\frac{\\|x_0-x^*\\|_2^2}{2t} + kt\\sigma^2$$\n",
    "\n",
    "and thereby\n",
    "\n",
    "$$\\mathbb{E}\\frac{1}{k}\\sum_{i=0}^{k-1} f(x_{i+1}) \\le f(x^*) + \\frac{\\|x_0-x^*\\|_2^2}{2tk} + t\\sigma^2.$$\n",
    "\n",
    "Jensen's inequality, i.e., convexity, now gives us\n",
    "\n",
    "$$\\mathbb{E} f(\\overline x_k) = \\mathbb{E} f(\\frac{1}{k}\\sum_{i=0}^{k-1}x_{i+1}) \\le f(x^*) + \\frac{\\|x_0-x^*\\|_2^2}{2tk} + t\\sigma^2.$$\n",
    "\n",
    "\n",
    "We will leave the \"number of iterations\" part of the result as an exercise.\n",
    "\n",
    "$\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf93e6-a305-418c-bd91-0906e2e26c2a",
   "metadata": {},
   "source": [
    "````{note}\n",
    "\"Hard\" convergence results of SGD usually require a non-constant step size, e.g., $\\mbox{step size} = \\frac{1}{\\mbox{current iteration}}$.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba4cdb-fb8e-492c-b9a0-357afc4fabbe",
   "metadata": {},
   "source": [
    "## The bias–variance trade-off\n",
    "\n",
    "We assume a regression dataset\n",
    "\n",
    "$$\\mathscr{D} =\\{(x_i,y_i)\\}_{i=1}^N \\subset \\mathbb{R}^n \\times \\mathbb{R}$$\n",
    "\n",
    "of realizations $(x_i, y_i)\\sim p_{X,Y}$.\n",
    "\n",
    "And we assume that the labels, $y$, are not deterministicly determined by the features $x$.\n",
    "As this means that we can have mutliple labels, it makes sense to think about the expected label for a given feature vector $x$.\n",
    "\n",
    "* The **expected label** is defined as\n",
    "\n",
    "$$\\overline y(x) = \\mathbb{E}_{y|x} Y = \\int_\\mathbb{R} y\\ d\\mathbb{P}(y|x) = \\int_\\mathbb{R} y\\ p(y|x) dy.$$\n",
    "\n",
    "* We will now also define the trained model based on some deterministic (in practice you could use a fixed seed) approach $\\mathcal{A}$ and the dataset as\n",
    "\n",
    "$$f_\\mathcal{D} = \\mathcal{A}(\\mathcal{D}).$$\n",
    "\n",
    "\n",
    "* As $\\mathcal{D}$ is randomly drawen from $p_{X,Y}$ we can also think about the **expected model** we get from an approach $\\mathcal{A}$. The approach describes the whole process of producing the model. This includes the network architecture, initialization, optimizer, learning rate, ... We define the **expected model** as\n",
    "\n",
    "$$\\overline f = \\mathbb{E}_\\mathcal{D} f_\\mathcal{D} = \\int_{\\mathbb{R}^{(n+1) \\times N}} f_\\mathcal{D} \\ d\\mathbb{P}(\\mathcal{D}).$$\n",
    "\n",
    "\n",
    "\n",
    "* Having these concepts in place allows us to think about the **expected test error**\n",
    "\n",
    "$$\\mathbb{E}_{(x,y)\\sim p_{x,y}, \\mathcal{D}\\sim p_{X,Y}^N} \\left(f_\\mathcal{D}(x) - y\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce6f64-0122-4da8-beb0-ec1ddb3c1389",
   "metadata": {},
   "source": [
    "````{prf:theorem} Bias-variance decomposition\n",
    ":label: bv_thm\n",
    "We can decompose the expected test error in the following way:\n",
    "\n",
    "$$\\mathbb{E}_{\\substack{(x,y)\\sim p_{x,y}\\\\ \\mathcal{D}\\sim p_{X,Y}^N}} \\left(f_\\mathcal{D}(x) - y\\right)^2 = \\mathbb{E}_{x, \\mathcal{D}} (f_\\mathcal{D}(x) - \\overline f(x))^2 + \\mathbb{E}_x (\\overline f(x) - \\overline y(x))^2 + \\mathbb{E}_{x,y} (\\overline y(x) - y)^2.$$\n",
    "\n",
    "We can interpret these terms of the sum as follows:\n",
    "* $\\mathbb{E}_{x, D} (f_\\mathcal{D}(x) - \\overline f(x))^2$\n",
    "    + The **variance** our approach has for different datasets of size $N$ from $p_{X,Y}$.\n",
    "    + I.e., if we would have had a different dataset, how different would we expect our model to be.\n",
    "* $\\mathbb{E}_x (\\overline f(x) - \\overline y(x))^2$\n",
    "    + The **bias** our approach has for different datasets of size $N$ from $p_{X,Y}$.\n",
    "    + I.e., does our approach produce models with a systematic error, e.g., values that are always too large.\n",
    "* $\\mathbb{E}_{x,y} (\\overline y(x) - y)^2$\n",
    "     + This is the **noise** inherent to the label due to its non-deterministic nature w.r.t. the features.\n",
    "    + I.e., the stuff that is impossible to know.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af109ff1-d6c0-432f-b620-2af709bb156a",
   "metadata": {},
   "source": [
    "```{admonition} Click for proof.\n",
    ":class: dropdown\n",
    "````{prf:proof}\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x,y,D}\\left[f_{D}(x) - y\\right]^{2}\n",
    "= \\mathbb{E}_{x,y,D}\\left[\\left(f_{D}(x) - \\overline f(x)\\right) + \\left(\\overline f(x) - y\\right)\\right]^{2}\\nonumber$$\n",
    "$$\n",
    "= \\mathbb{E}_{x, D}(\\overline f_{D}(x) - \\overline f(x))^{2} + \\mathbb{E}_{x, y} \\left(\\overline f(x) - y\\right)^{2} + 2 \\mathbb{E}_{x, y, D} \\left(f_{D}(x) - \\overline f(x)\\right)\\left(\\overline f(x) - y\\right)\n",
    "$$\n",
    "\n",
    "By splitting up $\\mathbb{E}_{x, y, D} = \\mathbb{E}_{x, y}\\mathbb{E}_{D}$ we see that the last term vanishes.\n",
    "\n",
    "Now we only need to show that the second term decomposes into the desired outcome.\n",
    "By expanding we find\n",
    "\n",
    "$$\\mathbb{E}_{x,y} (\\overline f(x) - y)^2 = \\mathbb{E}_{x,y} \\left[(\\overline f(x) - \\overline y(x)) + (\\overline y(x) - y)\\right]^2$$\n",
    "$$= \\mathbb{E}_{x,y} (\\overline f(x) - \\overline y(x))^2 + \\mathbb{E}_{x,y} (\\overline y(x) - y)^2 + 2\\mathbb{E}_{x,y} [\\overline f(x) -\\overline y(x)][\\overline y(x) - y].$$\n",
    "\n",
    "Again, by decomposing an expectation, this time $\\mathbb{E}_{x,y} = \\mathbb{E}_{x}\\mathbb{E}_{y|x}$, we see that the last term vanishes. $\\square$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4eca9-990e-4bd0-874b-e13f9d9ff54f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Having a high variance and low bias tends to lead to overfitting, while having a low variance and high bias tends to lead to underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8fbd7-37f8-4333-8f50-faffb17abff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "```{figure} images/bullseye.png\n",
    "---\n",
    "height: 400px\n",
    "---\n",
    "Bias vs variance. [Source.](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "```\n",
    "\n",
    "```{figure} images/biasvariance.png\n",
    "---\n",
    "height: 350px\n",
    "---\n",
    "The bias-variance trade-off. On the left, the model underfits; on the right, it overfits. [Source.](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "```\n",
    "\n",
    "```{figure} images/over_under_fitting.png\n",
    "---\n",
    "height: 200px\n",
    "---\n",
    "Over-, and under fitting for a one-dimensional regression problem. [Source.](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "```\n",
    "\n",
    "```{figure} images/over_under_fitting_classification.jpg\n",
    "---\n",
    "height: 200px\n",
    "---\n",
    "Over-, and under fitting for a two-dimensional classification problem. [Source.](https://datahacker.rs/018-pytorch-popular-techniques-to-prevent-the-overfitting-in-a-neural-networks/)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798dfce-90ba-43c7-a58a-c055269edadb",
   "metadata": {},
   "source": [
    "````{note}\n",
    "Recently people started to investigate new thinking frameworks about generalization and the bias-variance trade-off. These efforts are motivated by currently not understood observed phenomena in Deep Learning.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790474b-4003-47d0-a3b4-19f10850900c",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### SGD\n",
    "* A common question in DL is: If I increase my step size by a factor of $\\alpha$, how should I change my batch size to get a similar behavor in my loss? Sometimes the answer is: You have to keep the variance of your stochastic gradient update constant to get the same result. Thanks to the limit theorem this means you get similar behavor if you keep $\\frac{\\text{step size}}{\\sqrt{\\text{batch size}}}$ constant. If we use the theorem about the convergence of SGD and the central limit theorem, and try to answer the question in the limit in terms of the ball we converge into, we get the answer to keep $\\frac{\\text{step size}}{\\text{batch size}}$ constant. Derive both fractions.\n",
    "```{toggle}\n",
    "The first fraction results from looking at the variance of the stochastic gradient update:\n",
    "\n",
    "$$\\text{var}(t\\frac{1}{\\text{batch size}}\\sum_i g_i) = \\frac{t^2}{n}\\sum_i\\text{var}(g_i),$$\n",
    "\n",
    "here $g_i$ are stochastic gradients.\n",
    "\n",
    "The second fraction we get if we look at\n",
    "\n",
    "$$\\mathbb{E} f(\\overline x_k) \\le f(x^*) + \\frac{\\|x_0-x^*\\|_2^2}{2tk} + \\frac{t\\sigma^2}{2}$$\n",
    "\n",
    "and consider that central limit theorem tells us that $\\sigma^2\\propto \\frac{1}{\\text{batch size}}$.\n",
    "\n",
    " This means that if we want to converge into a ball given by $\\frac{t\\sigma^2}{2}$ of the same size, we have the scaling law that the size of the ball does not change if $\\frac{\\text{step size}}{\\text{batch size}}$ stays constant. This obviously only holds for $t L \\le 1$.\n",
    "\n",
    "```\n",
    "* What advantage does SGD have over GD from a compute/memory point of view?\n",
    "* What advantage does SGD w.r.t. to instable stationary points.\n",
    "* Train simple MLPs on MNIST.\n",
    "    1. One for each of the following step size and batch size pairs: $(1e-4, 2)$, $(5e-4, 10)$, $(1e-3, 20)$, $(5e-4, 50)$, $(1e-3, 200)$. Keep all other parameters constant.\n",
    "    2. Create of contraining the training loss curves of all the trained models.\n",
    "    3. Analyzte the two losses curve sets given by the relationships $\\frac{\\mbox{step size}}{\\mbox{batch size}}=\\mbox{const}$ and $\\frac{\\mbox{step size}}{\\sqrt{\\mbox{batch size}}}=\\mbox{const}$.\n",
    "* Show the \"number of iterations\" part of the theorem.\n",
    "\n",
    "```{toggle}\n",
    "We want a bound on $\\epsilon/2$. We begin with\n",
    "\n",
    "$$\\mathbb{E} f(\\overline x_k) - f(x^*) \\le \\epsilon/2 \\le \\frac{\\|x_0-x^*\\|_2^2}{2tk} + t\\sigma^2.$$\n",
    "\n",
    "This gives\n",
    "\n",
    "$$t\\epsilon \\le \\frac{\\|x_0-x^*\\|_2^2}{k} + 2t^2\\sigma^2$$\n",
    "\n",
    "and by plugging in $t=1/\\sqrt{k}$ we get\n",
    "\n",
    "$$\\frac{1}{\\sqrt{k}}\\epsilon \\le \\frac{\\|x_0-x^*\\|_2^2}{k} + \\frac{2}{k}\\sigma^2$$\n",
    "\n",
    "$$\\sqrt{k} \\le \\frac{\\|x_0-x^*\\|_2^2 + 2\\sigma^2}{\\epsilon}.$$\n",
    "\n",
    "Squaring both sides completes the proof.\n",
    "```\n",
    "\n",
    "### Bias-variance\n",
    "* Name two things that can cause overfitting.\n",
    "```{toggle}\n",
    "1. Too little trainig data.\n",
    "2. A too large/complex model.\n",
    "```\n",
    "* Complete the missing parts of the Bias-variance decomposition proof.\n",
    "* Would a linear regression under- or overfit on MNIST?\n",
    "* How can the split into a training, validation, and test set help to avoid overfitting?\n",
    "* Train an MLP on MNIST that overfits.\n",
    "* Train an MLP on MNIST that underfits.\n",
    "* When fitting a general polynomial of degree $p$ to data, how does the degree of the polynomial influence over- and underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef26602-b098-489c-aeec-f93fda3052b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
